import pandas as pd
import boto3
import subprocess
from datetime import datetime,timedelta,date
import json,csv
import os,sys
import glob
from configparser import ConfigParser
today = date.today()
today = today.strftime("%d-%m-%Y")
# # reading command line arguments when running resource_report.py
ar_lst = list(sys.argv)
# # #read the list of all the environment variable
# list_dbinstance = ar_lst[1]
home_path=os.environ.get("HOME")
eks_report = home_path+"/builds/Wtc2G1gT4/0/hmno-devops/weekly-audit/eks_report_"+today+".csv"
with open(eks_report, 'w', newline='') as file:
        writer = csv.writer(file)
        field = ["EKScluster","Restart count","Resource count"]
        writer.writerow(field)
rds_reports = home_path+"/builds/Wtc2G1gT4/0/hmno-devops/weekly-audit/rds_report_"+today+".csv"
with open(rds_reports, 'a', newline='') as file:
            writer = csv.writer(file)
            field = ["Instancename","Region","TotalStorage","Utilisedstorage"]
            writer.writerow(field)
list_env = ar_lst[1]
for env_name in list_env.split(','):
    print(env_name)
    prop_file = f"{home_path}/builds/Wtc2G1gT4/0/hmno-devops/weekly-audit/ENV/{env_name}-config.properties"
    print(prop_file)
    config = ConfigParser()
    config.read(prop_file)
    aws_profile = config.get("Profile","AWS_PROFILE")
    print(aws_profile)
    list_cluster = config.get("EKS-cluster","cluster")
    if config.has_section("db-instance"):
       list_instance = config.get("db-instance","dbinstance")
       print(list_instance)
       for db_instance in list_instance.split(','):
         print(db_instance)
         session = boto3.Session(profile_name=aws_profile)
         rds_client = session.client('rds')
         response = rds_client.describe_db_instances(DBInstanceIdentifier=db_instance)
         for instance in response['DBInstances']:
            if instance['DBInstanceIdentifier'] == db_instance:
              region= instance['AvailabilityZone'][:-1]
              allocatedstorage_space = response['DBInstances'][0]['AllocatedStorage']
              print (allocatedstorage_space)
              cloudwatch = session.client('cloudwatch',region_name=region)
              response = cloudwatch.get_metric_data(
                MetricDataQueries=[
                    {
                        'Id': 'fetching_FreeStorageSpace',
                        'MetricStat': {
                            'Metric': {
                                'Namespace': 'AWS/RDS',
                                'MetricName': 'FreeStorageSpace',
                                'Dimensions': [
                                    {
                                        "Name": "DBInstanceIdentifier",
                                        "Value": db_instance
                                    }
                                ]
                            },
                            'Period': 604800,
                            'Stat': 'Maximum'
                        }
                    }
                ],
                StartTime=(datetime.now() - timedelta(seconds=201600 * 3)).timestamp(),
                EndTime=datetime.now().timestamp(),
                ScanBy='TimestampDescending'
            )
            #FreeStorageSpace =response['MetricDataResults'][0]['Values'][0]

            bytes_to_gb_lambda = lambda bytes_value : bytes_value / (1000 ** 3)
            FreeStorageSpace = bytes_to_gb_lambda(response['MetricDataResults'][0]['Values'][0])
            print(FreeStorageSpace)
            utilisedstorage_space= allocatedstorage_space - FreeStorageSpace
            print(utilisedstorage_space)
            with open(rds_reports, 'a', newline='') as file:
                        writer = csv.writer(file)
                        row_list = [[db_instance, region,allocatedstorage_space,utilisedstorage_space]]
                        writer.writerows(row_list)
    else:
         print("db instance is not available")

    # Define cltr_resource before the loop
    cltr_resource = ""                    
# EKS cluster resource reports generation
for clustername in list_cluster.split(','):
    print(clustername)
    kube_config = home_path + "/" + clustername + "/.kube/config"
    os.environ['KUBECONFIG'] = kube_config
 
    cltr_resource = f"{clustername}_{today}.csv"  # Update cltr_resource inside the loop
 
    # Execute kubectl get pods -A
    kubectl_command_pods = ["kubectl", "get", "pods", "-A"]
    kubectl_process_pods = subprocess.Popen(kubectl_command_pods, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    kubectl_output_pods, _ = kubectl_process_pods.communicate()
    cmd_pods_output = kubectl_output_pods.strip().split("\n")
    pods_data = [line.split()[:5] for line in cmd_pods_output]
 
    # Write the data to the CSV file
    with open(cltr_resource, "w", newline='') as cltrfile:
        csv_writer = csv.writer(cltrfile)
        csv_writer.writerows(pods_data)
 
    # Execute kubectl get nodes -A
    kubectl_command_nodes = ["kubectl", "get", "nodes", "-A"]
    kubectl_process_nodes = subprocess.Popen(kubectl_command_nodes, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    kubectl_output_nodes, _ = kubectl_process_nodes.communicate()
    cmd_nodes_output = kubectl_output_nodes.strip().split("\n")
    nodes_data = [line.split()[:5] for line in cmd_nodes_output]
 
    # Append the data to the existing CSV file
    with open(cltr_resource, "a", newline='') as cltrfile:
        csv_writer = csv.writer(cltrfile)
        csv_writer.writerows(nodes_data)
 
    # Execute kubectl get hpa -A
    kubectl_command_hpa = ["kubectl", "get", "hpa", "-A"]
    kubectl_process_hpa = subprocess.Popen(kubectl_command_hpa, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    kubectl_output_hpa, _ = kubectl_process_hpa.communicate()
    cmd_hpa_output = kubectl_output_hpa.strip().split("\n")
    hpa_data = [line.split()[:5] for line in cmd_hpa_output]
 
    # Append the data to the existing CSV file
    with open(cltr_resource, "a", newline='') as cltrfile:
        csv_writer = csv.writer(cltrfile)
        csv_writer.writerows(hpa_data)
 
    print(f"CSV file '{cltr_resource}' generated successfully with pods, nodes, and hpa data.")
    print("#############################################################################")
    #reports = home_path+"/weekly-audit/"+cltr_resource
    #print(reports)
    #csv_path = "D:/Charter-SRE/Audit-scripts/"+env_name +".csv"
        df=pd.read_csv(cltr_resource)
        restart_count=df['RESTARTS'].sum()
    #restart_count=df['CPU'].sum()
        print("restart count", restart_count)
        resource_count = len(df['NAMESPACE'])
        print("resource count", resource_count)
    # append env_name, resource_count, restart_count into the resource_report csv file
        with open(eks_report, 'a', newline='') as file:
            writer = csv.writer(file)
            row_list = [[clustername, restart_count, resource_count]]
            writer.writerows(row_list)
print("upload all the csv files into s3 bucket")
#upload the csv files into s3 bucket
home_path=os.environ.get("HOME")
reports_path = f"/home/gitlab-runner/builds/Wtc2G1gT4/0/hmno-devops/weekly-audit/*.csv"
print(reports_path)
BUCKET_NAME = 'devopssit-misc-artifacts'
FOLDER_NAME = 'weekly-metrics-reports/'+today
session = boto3.Session(profile_name='nsl-devopssit')
s3 = session.client('s3')
csv_files = glob.glob(reports_path)
for filename in csv_files:
    key = "%s/%s" % (FOLDER_NAME, os.path.basename(filename))
    print("Putting %s as %s" % (filename,key))
    s3.upload_file(filename, BUCKET_NAME,key)
object_url="https://devopssit-misc-artifacts.s3.amazonaws.com/weekly-metrics-reports/"+today
print(object_url)
print("All_Done")
